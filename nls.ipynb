{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel Lyrics Synthesis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danieltjw/novel-lyrics-synthesis/blob/master/nls.ipynb)\n",
    "\n",
    "In this project, Recurrent Neural Networks (RNNs) are applied to sequence modeling and Natural Language Processing (NLP) tasks. Character-level language models were trained on 100 song lyrics and then used to generate new lyrics. The quality of the generated lyrics were evaluated using 3 metrics—ability to form valid words, emulate the original sentence structure (frequency distribution of sentence length) and similarity (BLEU score).\n",
    "\n",
    "https://danieltjw.github.io/novel-lyrics-synthesis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Sections\n",
    "\n",
    "1. Data\n",
    "\n",
    "2. Auxiliary\n",
    "\n",
    "3. Training\n",
    "\n",
    "4. Evaluation\n",
    "\n",
    "5. Synthesis\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "#### A. TensorFlow with GPU support\n",
    "\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "#### B. num2words library\n",
    "\n",
    "https://github.com/savoirfairelinux/num2words\n",
    "\n",
    "- `!pip install num2words`\n",
    "\n",
    "#### C. Song lyrics\n",
    "\n",
    "csv file (with 'Lyrics' header)\n",
    "\n",
    "- Location: /data/100_songs.csv\n",
    "\n",
    "#### D. SCOWL wordlist\n",
    "\n",
    "https://github.com/en-wl/wordlist\n",
    "\n",
    "- Location: /downloads/SCOWL_words_50.txt\n",
    "\n",
    "### Code References\n",
    "\n",
    "LSTM Text Generation: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. Libraries \"\"\"\n",
    "!pip install num2words\n",
    "# !pip install keras\n",
    "# !pip install nltk\n",
    "\n",
    "# Replaced 100_songs.csv with copyright free sherlock.csv\n",
    "!wget https://raw.githubusercontent.com/danieltjw/novel-lyrics-synthesis/master/data/sherlock.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. Data \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, OrderedDict as odict\n",
    "from num2words import num2words\n",
    "\n",
    "VER = 1.0\n",
    "DEFAULT_MAXLEN = 50 # default network sequence length\n",
    "\n",
    "def process_text(corpus):\n",
    "    \"\"\"Process text, aiming to reduce vocabulary\"\"\"\n",
    "    # This replaces strings according to a list of tuple\n",
    "    # Ordering is important\n",
    "    old_new = [(\"‘\",\"'\"),(\"’\",\"'\"),(\"“\",'\"'),(\"”\",'\"'),(\"!\",\"\"),(\"?\",\"\"),\n",
    "               ('\"',\"\"),(\":\",\" \"),(\" \\n\",\"\\n\"),(\";\",\",\"),\n",
    "               ('[hook]','[chorus]'),\n",
    "               (\"2am\",\"2 a.m.\"),(\"'45\",\"45\"),(\"9th\",\"nineth\"),\n",
    "               (\"in'\\n\",\"ing\\n\"),(\"in',\",\"ing,\"),(\"in' \",\"ing \"),\n",
    "               (\"'til\\n\",\"'till\\n\"),(\"'til,\",\"'till,\"),(\"'til \",\"'till \")]\n",
    "    for (old, new) in old_new:\n",
    "        corpus = corpus.replace(old, new)\n",
    "\n",
    "    # removes quotes around words\n",
    "    # {'I'm alive'} -> {I'm alive}\n",
    "    corpus = re.sub(r'([\\s\\n])\\'(.*?)\\'([\\s\\n])', r'\\1\\2\\3', corpus)\n",
    "    # removes embellishments: vocalisation\n",
    "    # {(ah-aah-aah)} -> {}\n",
    "    corpus = re.sub(r'\\(.*?\\)', r'', corpus)\n",
    "    # {apart,now} -> {apart, now}\n",
    "    corpus = re.sub(r'([a-z]+,)([a-z]+)', r'\\1 \\2', corpus)\n",
    "    # converts numbers from its numeric to alphabetic form\n",
    "    # {45} -> {forty-five}\n",
    "    corpus = re.sub(r'\\d+', lambda x: num2words(int(x.group())), corpus)\n",
    "    corpus = re.sub(r\"\\[[a-z]+\\][\\n]\", r'', corpus)\n",
    "    return corpus\n",
    "\n",
    "def word_token(sentence, output):\n",
    "    \"\"\"Returns token words, removes comma\"\"\"\n",
    "    for token in sentence.split():\n",
    "        if token.endswith(','):\n",
    "            token = token[:-1]\n",
    "        if len(token) > 0:\n",
    "            if isinstance(output, (set,)):\n",
    "                output.add(token)\n",
    "            elif isinstance(output, (list,)):\n",
    "                output.append(token)\n",
    "    return output\n",
    "\n",
    "def print_data_state(data):\n",
    "    \"\"\"Prints summary of data\"\"\"\n",
    "    print('\\nData state:')\n",
    "    for key in data:\n",
    "        if key in ['maxlen', 'test_size', 'random_state']:\n",
    "            print(key, ':', data[key]) \n",
    "        elif 'counter' in key:\n",
    "            print(key, ':', sum(data[key].values()))\n",
    "        else:\n",
    "            print(key, ':', len(data[key]))\n",
    "\n",
    "def get_chars_index(string, chars='\\n'):\n",
    "    \"\"\"Gets index of specific chars in string\"\"\"\n",
    "    chars_index = []\n",
    "    for index, char in enumerate(string):\n",
    "        if char in chars:\n",
    "            chars_index.append(index)\n",
    "    return chars_index\n",
    "\n",
    "def valid_dist_between_chars(string, chars='\\n'):\n",
    "    \"\"\"Returns count of full sentences, excluding sentences of len 0\"\"\"\n",
    "    chars_index = get_chars_index(string)\n",
    "    dist = [chars_index[index_index+1]-char_index for index_index, char_index in enumerate(chars_index[:-1])]\n",
    "    valid_dist = [dis for dis in dist if dis>1]\n",
    "    return valid_dist\n",
    "\n",
    "def form(value, dec=2):\n",
    "    if isinstance(value, float):\n",
    "        format_str = '{:0.'+str(dec)+'f}'\n",
    "        return format_str.format(round(value, dec))\n",
    "    return str(value)\n",
    "\n",
    "def describe_sentence(sentences, maxlen):\n",
    "    \"\"\"Gets likelihood of seeing a full sentence\"\"\"\n",
    "    sent_count = 0\n",
    "    for sent in sentences:\n",
    "        valid_dist_list = valid_dist_between_chars(sent)\n",
    "        sent_count += len(valid_dist_list)\n",
    "    print('\\nMean chance an input will contain a full sentence(s) (', \n",
    "          'maxlen: ', maxlen, '): ', form(sent_count/len(sentences)*100)+'%', sep='')\n",
    "    \n",
    "def data_vector_split(data, maxlen=DEFAULT_MAXLEN, test_size=0.01, random_state=42):\n",
    "    \"\"\"Vectorises the data and then splits it into train / test sets\"\"\"\n",
    "    # if the data configuration is similar from a previous call, skip method\n",
    "    if 'maxlen' in data:\n",
    "        if data['maxlen'] == maxlen and data['test_size'] == test_size and data['random_state'] == random_state:\n",
    "            return           \n",
    "        \n",
    "    charset = data['charset']\n",
    "    char_indices = dict((c, i) for i, c in enumerate(charset))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(charset))\n",
    "\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    songs = data['songs']\n",
    "    # cuts corpus into sequences of maxlen characters, song-wise\n",
    "    # ensures no bleeding of lyrics between adjacent songs\n",
    "    for song in songs:\n",
    "        for i in range(0, len(song) - maxlen, 1):\n",
    "            sentences.append(song[i: i + maxlen])\n",
    "            next_chars.append(song[i + maxlen])\n",
    "    print('Num sequences:', len(sentences))\n",
    "    describe_sentence(sentences, maxlen)\n",
    "    print('Vectorisation...')\n",
    "    X = np.zeros((len(sentences), maxlen, len(charset)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(charset)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "    # splits data into train / test set\n",
    "    # random state can be used to introduce variation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    # 'data' dictionary object is used to hold all dataset related data:\n",
    "    #  - train / test dataset: X_train, y_train, X_test, y_test\n",
    "    #  - data configuration: maxlen, test_size, random_state\n",
    "    #  - vectorisation configuration: char_indices, indices_char\n",
    "    for name in ['X_train','y_train','X_test','y_test', \n",
    "                 'maxlen', 'test_size', 'random_state', \n",
    "                 'char_indices', 'indices_char']:\n",
    "        data[name] = eval(name)\n",
    "\n",
    "def describe_corpus(data):\n",
    "    \"\"\"Prints descriptive statistics of corpus\"\"\"\n",
    "    print('\\nDescribing each song len:')\n",
    "    print(pd.Series([len(song) for song in data['songs']]).describe())\n",
    "    \n",
    "    print('\\nDescribing each line len:')\n",
    "    # len of sentence is increased by 1, to represent an additional newline char\n",
    "    # this brings it in line with the 2nd metric being evaluated\n",
    "    each_line_len = pd.Series([len(line)+1 for line in data['lines_counter'].elements()])\n",
    "    print(each_line_len.describe())\n",
    "#     print(\"The line of len\", max(each_line_len), ':')\n",
    "#     [print(line) for line in data['lines_counter'] if len(line) == max(each_line_len)]\n",
    "    \n",
    "    print('\\nDescribing each word len:')\n",
    "    each_word_len = pd.Series([len(word) for word in data['words_counter'].elements()])\n",
    "    print(each_word_len.describe())\n",
    "#     print(\"The word of len\", max(each_word_len), ':', end=' ')\n",
    "#     [print(word) for word in data['words_counter'] if len(word) == max(each_word_len)]\n",
    "\n",
    "    print('\\nVocab:', len(data['charset']), data['charset'])\n",
    "\n",
    "def process_corpus(filename='data/100_songs.csv'):\n",
    "    \"\"\"Prints descriptive statistics of corpus\"\"\"\n",
    "    raw_corpus = pd.read_csv(filename)\n",
    "    print(raw_corpus.shape)\n",
    "    data = odict([\n",
    "        ('songs', []),\n",
    "        ('lines_counter', Counter()),\n",
    "        ('words_counter', Counter()),\n",
    "        ('chars_counter', Counter()),\n",
    "        ('wordset', None),\n",
    "        ('charset', None),\n",
    "        ('lines_token', [])\n",
    "    ])\n",
    "    songs = []\n",
    "    line_len = []\n",
    "    for index, song in enumerate(raw_corpus['Lyrics'].str.lower()):\n",
    "        song = process_text(song)\n",
    "        data['songs'].append(song)\n",
    "        data['chars_counter'].update(song)\n",
    "        lines = song.splitlines()\n",
    "        data['lines_counter'].update(lines)\n",
    "        for line in lines:\n",
    "            words = word_token(line, [])\n",
    "            if len(words) > 0:\n",
    "                data['words_counter'].update(words)\n",
    "                data['lines_token'].append(words)\n",
    "    del data['lines_counter']['']\n",
    "    data['wordset'] = set(data['words_counter'])\n",
    "    data['charset'] = sorted(list(data['chars_counter']))\n",
    "    data_vector_split(data)\n",
    "    return data\n",
    "\n",
    "data = process_corpus(filename='sherlock.csv')\n",
    "describe_corpus(data)\n",
    "print_data_state(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 2. Auxiliary \"\"\"\n",
    "\"\"\"\n",
    "Common methods called by Training and Evaluation\n",
    "Enables spreadsheet reports\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "def path_join(args):\n",
    "    return os.path.abspath(os.path.join(*args))\n",
    "\n",
    "def write_header(csv_writer, filepath):\n",
    "    if 'train' in filepath:\n",
    "        csv_writer.writerow(['val_loss', 'epoch', 'best_of', \n",
    "                   'type', 'batch_size', 'val_split', \n",
    "                   'maxlen', 'layer', 'unit', \n",
    "                   'train_start', 'train_dur', \n",
    "                   'filepath', 'history'])\n",
    "    elif 'eval' in filepath:\n",
    "        csv_writer.writerow(['div_1.0', \n",
    "                             'words', 'new', 'valid', 'invalid', 'chars',\n",
    "                             'sen_len_count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max',\n",
    "#                              'sen_len_ext_count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max',\n",
    "                             'bleu_count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max',\n",
    "                             'delim', 'mean_of', 'test_num',\n",
    "                   'eval_start', 'eval_dur', \n",
    "                   'filepath', 'result']) \n",
    "    elif 'run' in filepath:\n",
    "        csv_writer.writerow(['Start Run', 'best_of', 'para_combo', 'work_dir', 'para_range',\n",
    "                   'VER', 'Comments'])\n",
    "\n",
    "def write_row(output, output_file='saved_models/output.csv'):\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'a', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            write_header(csv_writer, output_file)\n",
    "    try:\n",
    "        with open(output_file, 'a', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(output)\n",
    "    except PermissionError:\n",
    "        # if file to be written to is open (locked), write to temporary file\n",
    "        # happens often enough to warrant this\n",
    "        print('WARNING !!! File Locked : ' + output_file)\n",
    "        with open(output_file.replace('.csv', '_locked.csv'), 'a', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(output)\n",
    "        \n",
    "def str_time(time):\n",
    "    return str(time)[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3. Training \"\"\"\n",
    "from keras import backend\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import CuDNNLSTM, LSTM, CuDNNGRU, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from glob import glob\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def rnn_model(para, data):\n",
    "    \"\"\"Creates a new RNN model\"\"\"\n",
    "    model_call = {\n",
    "        'CuDNNLSTM': CuDNNLSTM,\n",
    "        'CuDNNGRU': CuDNNGRU,\n",
    "        'LSTM': LSTM,\n",
    "        'GRU': GRU    \n",
    "    }\n",
    "    maxlen = para['maxlen']\n",
    "    model_type = para['type']\n",
    "    unit = para['unit']\n",
    "    layer = para['layer']\n",
    "    charset = data['charset']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(model_call[model_type](unit, return_sequences=True if layer>1 else False, \n",
    "                   input_shape=(maxlen, len(charset))))\n",
    "    for lay in range(layer-1):\n",
    "        model.add(model_call[model_type](unit, return_sequences=False \n",
    "                       if lay == layer-2 else True))\n",
    "\n",
    "    model.add(Dense(len(charset)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def train_model(para, data, run, run_index):\n",
    "    \"\"\"Use hyper-parameters (para) to create and train a model\"\"\"\n",
    "    verbose = run['verbose']\n",
    "    backend.clear_session()\n",
    "    # 'train' dict holds information to be written to spreadsheet\n",
    "    train = odict([\n",
    "        ('val_loss', None),\n",
    "        ('epoch', None),\n",
    "        ('start_train', datetime.now())\n",
    "    ])\n",
    "    # naming convention used to organise directory structure\n",
    "    # and also uniquely identify a model's save file\n",
    "    model_name = '%s_bs-%d_vs-%.2f_ml-%d_layer-%d_unit-%d'%(tuple(para.values()))\n",
    "    filepath = run['work_dir'] + model_name + '.r' + str(run_index) + '.hdf5'\n",
    "    \n",
    "    model = rnn_model(para, data)\n",
    "    # callbacks\n",
    "    checkpointer = ModelCheckpoint(filepath, verbose=verbose, period=1, save_best_only=True)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=verbose)\n",
    "    # tensorboarder = TensorBoard('logs/' + model_name, histogram_freq=0)\n",
    "    callbacks = [checkpointer, early_stopper]\n",
    "\n",
    "    history = model.fit(data['X_train'], data['y_train'],\n",
    "                        batch_size=para['batch_size'],\n",
    "                        validation_split=para['val_split'],\n",
    "                        shuffle=True,\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    val_loss_list = history.history['val_loss']\n",
    "    min_index = val_loss_list.index(min(val_loss_list))\n",
    "    train['val_loss'] = form(val_loss_list[min_index], 5)\n",
    "    train['epoch'] = min_index+1\n",
    "    train['dur_train'] = str_time(datetime.now() - train['start_train'])\n",
    "    train['start_train'] = str_time(train['start_train'])\n",
    "    train['filepath'] = filepath\n",
    "    train['history'] = history.history\n",
    "    return train    \n",
    "\n",
    "def best_train(train_list):\n",
    "    \"\"\"Select the best model from candidates based on validation loss\"\"\"\n",
    "    train_list_vloss = [train['val_loss'] for train in train_list]\n",
    "    min_index = train_list_vloss.index(min(train_list_vloss))\n",
    "    for index, train in enumerate(train_list):\n",
    "        if train['epoch'] <= 5:\n",
    "            print(\"Low epoch detected:\", train['epoch'], train['filepath'])\n",
    "            write_row([json.dumps(train, sort_keys=True)])\n",
    "        if min_index == index:\n",
    "            # best candidate will be renamed as a finalisation step\n",
    "            old_filepath = train['filepath']\n",
    "            train['filepath'] = train['filepath'].partition('.r')[0] + '.hdf5'\n",
    "            os.rename(old_filepath, train['filepath'])\n",
    "        else:\n",
    "            # other candidates' save file will be deleted\n",
    "            os.remove(train['filepath'])\n",
    "    return train_list[min_index]\n",
    "\n",
    "def str_para_range(para_range):\n",
    "    \"\"\"Generates the directory name based on the hyper-parameters range\"\"\"\n",
    "    abbrev = {'type': '',\n",
    "              'batch_size': 'bs', \n",
    "              'val_split': 'vs',\n",
    "              'maxlen': 'ml',\n",
    "              'layer': 'layer',\n",
    "              'unit': 'unit'}\n",
    "    para_str = ''\n",
    "    for key in para_range:\n",
    "        para_str += abbrev[key] + '-' + str('-'.join(map(str, para_range[key])))\n",
    "        para_str += '_'\n",
    "    para_str += 'v'+str(VER)+'/'\n",
    "    return para_str[1:]\n",
    "\n",
    "def pre_model_exist(para, run):\n",
    "    \"\"\"If a finalised saved file exist, skip model training\"\"\"\n",
    "    model_name = '%s_bs-%d_vs-%.2f_ml-%d_layer-%d_unit-%d'%(tuple(para.values()))\n",
    "    filepath = run['work_dir'] + '/' + model_name + '.hdf5'\n",
    "    if os.path.exists(filepath):\n",
    "        print('Skipping pre-existing model: ' + os.path.basename(filepath))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def run_train(para_range, run=None, data=data):\n",
    "    \"\"\"Iterates through all combination of the hyper-parameter ranges\"\"\"\n",
    "    default_train_run = odict([\n",
    "        ('verbose', 0),\n",
    "        ('work_dir', 'saved_models/' + str_para_range(para_range)),\n",
    "        ('best_of', 10)])\n",
    "    if run is None:\n",
    "        run = default_train_run\n",
    "    # all possible combination of parameters\n",
    "    para_combo = list(itertools.product(*[para_range[key] for key in para_range]))\n",
    "    # logs training run in \"saved_models/run.csv\"\n",
    "    write_row([str_time(datetime.now()), run['best_of'], len(para_combo), \n",
    "               run['work_dir'], json.dumps({'para_range': para_range}), VER],\n",
    "          path_join(['saved_models/', 'run.csv']))\n",
    "    os.makedirs(run['work_dir'], exist_ok=True)\n",
    "    for model_index, par in enumerate(para_combo, 1):\n",
    "        para = odict()\n",
    "        for index, key in enumerate(para_range):\n",
    "            para[key] = par[index]\n",
    "        if pre_model_exist(para, run):\n",
    "            continue\n",
    "        else:\n",
    "            print('Start Training :', model_index, '/', len(para_combo), \n",
    "                  list(para.values()), '(', run['best_of'], 'Runs )')\n",
    "        train_list = []\n",
    "        for run_index in range(run['best_of']):\n",
    "            data_vector_split(data, maxlen=para['maxlen'], test_size=0.001, random_state=42)\n",
    "            train = train_model(para, data, run, run_index)\n",
    "            print('Run', run_index+1, ':', train['dur_train'], '> ' , end=\"\")\n",
    "            write_row(list(train.values())[:2] + [run['best_of']] + list(para.values()) \n",
    "                      + list(train.values())[2:5] + [json.dumps(train['history'], sort_keys=True)], \n",
    "                      path_join([run['work_dir'], 'train_candidate.csv']))\n",
    "            train_list.append(train)\n",
    "        train = best_train(train_list)\n",
    "        write_row(list(train.values())[:2] + [run['best_of']] + list(para.values()) \n",
    "                  + list(train.values())[2:5] + [json.dumps(train['history'], sort_keys=True)], \n",
    "                  path_join([run['work_dir'], 'train.csv']))\n",
    "        print('Done')\n",
    "    return run['work_dir'] + 'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 4. Evaluation \"\"\"\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "MAX_GEN_LEN = 500\n",
    "\n",
    "def create_dictset(data, filename='downloads/SCOWL_words_50.txt'):\n",
    "    \"\"\"Create dictset for valid words eval\"\"\"\n",
    "    # words of len 1-2 were manually screened to \n",
    "    # exclude words that don't appear in lyrics\n",
    "    add_words = set(['a', 'i', 'ah', 'am', 'an', 'as', 'at',\n",
    "                 'be', 'by', 'do', 'ex', 'go', 'he', 'hi',\n",
    "                 'if', 'in', 'is', 'it', 'ma', 'me', 'my', \n",
    "                 'no', 'of', 'oh', 'on', 'or', 'pa', 'so', \n",
    "                 'to', 'uh', 'um', 'up', 'us', 'we'])\n",
    "    scowl_words_in = open(filename).read().splitlines()\n",
    "    scowl_words = set()\n",
    "    for word in scowl_words_in:\n",
    "        if len(word) > 2:\n",
    "            scowl_words.add(word.lower())  \n",
    "    scowl_words.union(add_words)\n",
    "    data['dictset'] = set.union(*[data['wordset'], scowl_words])\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample an index from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def evaluate_generated(generated, dictset, wordset, result_div):\n",
    "    \"\"\"\n",
    "    Evaluate generated sentence\n",
    "    - 1st metric: valid word %\n",
    "    - 3rd metric: sentence BLEU score\n",
    "    \"\"\"\n",
    "    generated_words = word_token(generated, [])\n",
    "    bleu_score = sentence_bleu(data['lines_token'], generated_words, \n",
    "                          smoothing_function=SmoothingFunction().method3)\n",
    "    result_div['bleu'].append(bleu_score)\n",
    "    for word in generated_words:\n",
    "        if word not in dictset:\n",
    "            result_div['invalid_words_counter'].update([word])\n",
    "            result_div['total_invalid_words'] += 1\n",
    "        else:\n",
    "            result_div['valid_words_counter'].update([word])\n",
    "            result_div['total_valid_words'] += 1\n",
    "            if word not in wordset:\n",
    "                result_div['new_words_counter'].update([word])\n",
    "                result_div['total_new_words'] += 1\n",
    "    result_div['total_words'] += len(generated_words)\n",
    "\n",
    "def print_verbose_2(div, index, x_pred_sen, generated):\n",
    "    print('.'*42)\n",
    "    print('Phrase:', index+1, '|', 'Diversity:', div)\n",
    "    print('*' + x_pred_sen + '*' + generated)\n",
    "    \n",
    "def print_verbose_1(div, X_test, mean_of, delim, result):\n",
    "    print('-'*42)\n",
    "    print('Diversity:', div, '| mean_of:', mean_of, \n",
    "          '| test_num:', len(X_test), '| delim:', delim)\n",
    "    total_invalid_words = result[div]['total_invalid_words']\n",
    "    print(total_invalid_words, 'invalid words:', 'None' if total_invalid_words == 0 \n",
    "          else result[div]['invalid_words_counter'])\n",
    "    total_new_words = result[div]['total_new_words']\n",
    "    print(total_new_words, 'new words:', 'None' if total_new_words == 0 \n",
    "          else result[div]['new_words_counter'])\n",
    "    print('Total valid words:', result[div]['total_valid_words'], '/', result[div]['total_words'], \n",
    "      '(' + form(result[div]['total_valid_words_per']) + '%)')\n",
    "    print('BLEU score:', sum(result[div]['bleu']) / len(result[div]['bleu']) )\n",
    "    print('='*42)\n",
    "#     print('average word length:', result[div]['total_chars_evaluated']/result[div]['total_words'])\n",
    "\n",
    "def evaluate_model(model, para, data, run):\n",
    "    \"\"\"\n",
    "    Evaluate a model iteratively based on:\n",
    "    - diversity levels\n",
    "      - each test data point\n",
    "        - repeats (mean_of)\n",
    "          - sentence generation (character-by-character)\n",
    "          - evaluation of sentence\n",
    "    - return results\n",
    "    \"\"\"\n",
    "    maxlen = para['maxlen']\n",
    "    result = odict([\n",
    "        ('start_eval', datetime.now())\n",
    "    ])\n",
    "    for div in run['diversity']:\n",
    "        result[div] = odict([('new_words_counter', Counter()),\n",
    "                             ('valid_words_counter', Counter()),\n",
    "                             ('invalid_words_counter', Counter()),\n",
    "                             ('total_valid_words_per', 0),\n",
    "                             ('total_words', 0),\n",
    "                             ('total_new_words', 0),\n",
    "                             ('total_valid_words', 0),\n",
    "                             ('total_invalid_words', 0),\n",
    "                             ('total_chars_evaluated', 0),\n",
    "                             ('sen_len', []),\n",
    "#                              ('sen_len_ext', []),\n",
    "                             ('bleu', [])])\n",
    "\n",
    "        for test_index in range(len(data['X_test'])):\n",
    "            x_pred_prime = data['X_test'][test_index:test_index+1]\n",
    "            x_pred_sen = ''.join([data['indices_char'][np.argmax(x_pred_prime[0, i])] for i in range(maxlen)])\n",
    "            \n",
    "            for run_index in range(run['mean_of']):#\n",
    "                x_pred = x_pred_prime  \n",
    "                generated = ''\n",
    "                start_delim = -1 \n",
    "                end_delim = -1\n",
    "                for gen_index in range(MAX_GEN_LEN):\n",
    "                    preds = model.predict(x_pred)[0]\n",
    "                    next_index = sample(preds, div)\n",
    "                    next_char = data['indices_char'][next_index]\n",
    "                    generated += next_char\n",
    "                    # count num of delimiters in generated sentence so far\n",
    "                    delim_num = sum(map(generated.count, ['\\n',' ']))\n",
    "                    # returns len of full sentences for 2nd metric\n",
    "                    valid_sen_len = valid_dist_between_chars(''.join(x_pred_sen.rpartition('\\n')[1:])\n",
    "                                                             + generated)\n",
    "                    # tracks the start and end of sentences, used by 1st and 3rd metrics\n",
    "                    if start_delim==-1 and delim_num == 1:\n",
    "                        start_delim = gen_index\n",
    "                    if end_delim==-1 and delim_num == run['delim'] + 1:\n",
    "                        end_delim = gen_index\n",
    "                    # if the generated sentence is sufficient to calculate all 3 metrics, break\n",
    "                    if end_delim != -1 and len(valid_sen_len)>=1:\n",
    "                        result[div]['sen_len'] += valid_sen_len[:1]\n",
    "#                         result[div]['sen_len_ext'] +sen_len_ext= valid_sen_len[:1]\n",
    "                        break\n",
    "                    # converts the char generated into its vectorised form\n",
    "                    x_pred_next = np.zeros((1, 1, len(data['charset'])), dtype=np.bool)\n",
    "                    x_pred_next[0, 0, data['char_indices'][next_char]] = True\n",
    "                    # remove the first in sequence and append new char to end\n",
    "                    x_pred = np.concatenate((x_pred[:,1:maxlen], x_pred_next), axis=1)\n",
    "                    if gen_index == MAX_GEN_LEN - 1:\n",
    "#                         result[div]['sen_len_ext'] += [len(''.join(x_pred_sen.rpartition('\\n')[1:])) + MAX_GEN_LEN]\n",
    "                        print(\"pred len exceeded:\", MAX_GEN_LEN, \"| div:\", div)\n",
    "                # gets the generated sentence demarcated by the start and end delimiters\n",
    "                # this removes any partial words that began in the test data that may\n",
    "                # inflate the score\n",
    "                pro_generated = generated[start_delim+1:end_delim]\n",
    "                result[div]['total_chars_evaluated'] += len(pro_generated)\n",
    "                evaluate_generated(pro_generated, data['dictset'], data['wordset'], result[div])\n",
    "                if run['verbose'] >= 2:\n",
    "                    print_verbose_2(div, test_index, x_pred_sen, generated)\n",
    "        result[div]['total_valid_words_per'] = result[div]['total_valid_words']/result[div]['total_words']*100\n",
    "        if run['verbose'] >= 1:\n",
    "            print_verbose_1(div, data['X_test'], run['mean_of'], run['delim'], result)\n",
    "    result['dur_eval'] = str_time(datetime.now() - result['start_eval'])\n",
    "    result['start_eval'] = str_time(result['start_eval'])\n",
    "    return result\n",
    "\n",
    "def load_pre_model(filepath):\n",
    "    model = None\n",
    "    if os.path.exists(filepath):\n",
    "        print('Loading saved model: ' + os.path.basename(filepath))\n",
    "        model = load_model(filepath)\n",
    "    return model\n",
    "\n",
    "def get_para_from_filepath(filepath):\n",
    "    \"\"\"gets model hyper-parameter from model save filename\"\"\"\n",
    "    filepath = os.path.basename(filepath)\n",
    "    para = odict([\n",
    "    ('type', filepath.partition('_')[0]),\n",
    "    ('batch_size', int(filepath.partition('bs-')[2].partition('_')[0])),\n",
    "    ('val_split', float(filepath.partition('vs-')[2].partition('_')[0])),\n",
    "    ('maxlen', int(filepath.partition('ml-')[2].partition('_')[0])),\n",
    "    ('layer', int(filepath.partition('layer-')[2].partition('_')[0])),\n",
    "    ('unit', int(filepath.partition('unit-')[2].partition('.')[0]))\n",
    "    ])\n",
    "    return para\n",
    "\n",
    "def format_result(result, diversity):\n",
    "    for_result = []\n",
    "    for div in diversity:\n",
    "        for key in result[div]:\n",
    "            if 'total' in key:\n",
    "                for_result.append(form(result[div][key]))\n",
    "        sen_len_des = pd.Series(result[div]['sen_len']).describe()\n",
    "        for (_, value) in sen_len_des.items():\n",
    "            for_result.append(form(value))\n",
    "#         sen_len_ext_des = pd.Series(result[div]['sen_len_ext']).describe()\n",
    "#         for (_, value) in sen_len_ext_des.items():\n",
    "#             for_result.append(form(value))\n",
    "        bleu_des = pd.Series(result[div]['bleu']).describe()\n",
    "        for (_, value) in bleu_des.items():\n",
    "            for_result.append(form(value, 3))\n",
    "    return for_result\n",
    "\n",
    "def run_eval(train_csv, start_index=None, end_index=None, run=None, data=data):\n",
    "    \"\"\"\n",
    "    train_csv: filepath to the train.csv output by run_train method\n",
    "    start_index, end_index: optionally, only eval a subset of models\n",
    "    \"\"\"\n",
    "    default_eval_run = odict([\n",
    "        ('verbose', 0),\n",
    "        ('diversity', [1.0]),\n",
    "        ('mean_of', 5),\n",
    "        ('delim', 10)])\n",
    "    if run is None:\n",
    "        run = default_eval_run\n",
    "    if 'dictset' not in data:\n",
    "        create_dictset(data)\n",
    "    input_table = pd.read_csv(train_csv)\n",
    "    filepath_list = input_table['filepath'].tolist()[start_index:end_index]\n",
    "    # evaluate each model listed in train.csv\n",
    "    for file_index, filepath in enumerate(filepath_list):\n",
    "        backend.clear_session()\n",
    "        para = get_para_from_filepath(filepath)\n",
    "        data_vector_split(data, maxlen=para['maxlen'], test_size=0.001, random_state=42)\n",
    "        model = load_pre_model(filepath)\n",
    "        result = evaluate_model(model, para, data, run)\n",
    "        for_result = format_result(result, run['diversity'])\n",
    "        print('Done Evaluating :', file_index+1, '/', len(filepath_list), \n",
    "              [form(result[div]['total_valid_words_per']) for div in run['diversity']], \n",
    "              result['dur_eval'])\n",
    "        write_row(for_result + [run['delim'], run['mean_of'], len(data['X_test']), result['start_eval'], \n",
    "                  result['dur_eval'], filepath], #, json.dumps(result)],\n",
    "                  path_join([train_csv.replace('train.csv', 'eval_d' + str(run['delim']) + '.csv')]))\n",
    "    return train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 5. Synthesis \"\"\"\n",
    "\"\"\"Generates a sentence using a prime (seed)\"\"\"\n",
    "\n",
    "def prime_model(prime, maxlen, data):\n",
    "    \"\"\"Vectorise the priming string\"\"\"\n",
    "    charset = data['charset']\n",
    "    char_indices = data['char_indices']\n",
    "    indices_char = data['indices_char']\n",
    "    prime = prime[-maxlen:]\n",
    "    prime = (maxlen-len(prime))*' ' + prime\n",
    "    print('Prime Vectorisation...')\n",
    "    X = np.zeros((1, maxlen, len(charset)), dtype=np.bool)\n",
    "    for t, char in enumerate(prime):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    # replaces the test dataset with the priming sentence\n",
    "    data['X_test'] = X\n",
    "    # modifies the test_size to ensure data configuration info has changed\n",
    "    # to prevent issues with subsequent normal training / evaluation\n",
    "    data['test_size'] = 1.0\n",
    "    \n",
    "def run_prime(prime, train_csv, start_index=None, end_index=None, run=None, data=data):\n",
    "    \"\"\"\n",
    "    prime: string to seed synthesis\n",
    "    train_csv: filepath to the train.csv output by run_train method\n",
    "    start_index, end_index: optionally, only prime a subset of models\n",
    "    \"\"\"\n",
    "    prime = prime.lower()\n",
    "    if not all(char in data['charset'] for char in prime):\n",
    "        print('These chars are not in models\\' vocab: ', \n",
    "              [char for char in prime if char not in data['charset']])\n",
    "        return\n",
    "    default_prime_run = odict([\n",
    "        ('verbose', 2),\n",
    "        ('diversity', [1.0, 0.5]),\n",
    "        ('mean_of', 1),\n",
    "        ('delim', 30)])\n",
    "    if run is None:\n",
    "        run = default_prime_run\n",
    "    if 'dictset' not in data:\n",
    "        create_dictset(data)\n",
    "    input_table = pd.read_csv(train_csv)\n",
    "    filepath_list = input_table['filepath'].tolist()[start_index:end_index]\n",
    "    for file_index, filepath in enumerate(filepath_list):\n",
    "        backend.clear_session()\n",
    "        para = get_para_from_filepath(filepath)\n",
    "        prime_model(prime, para['maxlen'], data)\n",
    "        model = load_pre_model(filepath)\n",
    "        result = evaluate_model(model, para, data, run)\n",
    "        for_result = format_result(result, run['diversity'])\n",
    "        print('Done Priming :', file_index+1, '/', len(filepath_list), \n",
    "              [form(result[div]['total_valid_words_per']) for div in run['diversity']], \n",
    "              result['dur_eval'])\n",
    "    return train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Smoke Test\n",
    "para_range = odict([\n",
    "    ('type', ['CuDNNGRU']),\n",
    "    ('batch_size', [128]),\n",
    "    ('val_split', [0.05]),\n",
    "    ('maxlen', [5]),\n",
    "    ('layer', [2]),\n",
    "    ('unit', [221])\n",
    "])\n",
    "\n",
    "train_run = odict([\n",
    "    ('verbose', 0),\n",
    "    ('work_dir', 'saved_models/' + str_para_range(para_range)),\n",
    "    ('best_of', 1)])\n",
    "\n",
    "eval_run = odict([\n",
    "    ('verbose', 0),\n",
    "    ('diversity', [1.0]),\n",
    "    ('mean_of', 1),\n",
    "    ('delim', 10)])\n",
    "\n",
    "prime_run = odict([\n",
    "    ('verbose', 2),\n",
    "    ('diversity', [1.0, 0.5]),\n",
    "    ('mean_of', 1),\n",
    "    ('delim', 30)])\n",
    "\n",
    "expected = \"saved_models/CuDNNGRU_bs-128_vs-0.05_ml-5_layer-2_unit-221_v1.0/train.csv\"\n",
    "train_csv = run_train(para_range, train_run)\n",
    "train_csv = run_eval(train_csv, run=eval_run)\n",
    "assert expected == run_prime(\"dream\", train_csv, run=prime_run)\n",
    "\n",
    "# Extended Test\n",
    "# para_range = odict([\n",
    "#     ('type', ['CuDNNGRU']),\n",
    "#     ('batch_size', [128]),\n",
    "#     ('val_split', [0.05]),\n",
    "#     ('maxlen', [5]),\n",
    "#     ('layer', [1, 2, 3]),\n",
    "#     ('unit', [221])\n",
    "# ])\n",
    "\n",
    "# train_run = odict([\n",
    "#     ('verbose', 0),\n",
    "#     ('work_dir', 'saved_models/' + str_para_range(para_range)),\n",
    "#     ('best_of', 3)])\n",
    "\n",
    "# eval_run = odict([\n",
    "#     ('verbose', 0),\n",
    "#     ('diversity', [1.0]),\n",
    "#     ('mean_of', 3),\n",
    "#     ('delim', 10)])\n",
    "\n",
    "# prime_run = odict([\n",
    "#     ('verbose', 2),\n",
    "#     ('diversity', [1.0, 0.5]),\n",
    "#     ('mean_of', 1),\n",
    "#     ('delim', 30)])\n",
    "\n",
    "# expected = \"saved_models/CuDNNGRU_bs-128_vs-0.05_ml-5_layer-1-2-3_unit-221_v1.0/train.csv\"\n",
    "# train_csv = run_train(para_range, train_run)\n",
    "# train_csv = run_eval(train_csv, run=eval_run)\n",
    "# assert expected == run_prime(\"dream\", train_csv, run=prime_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Model\n",
    "# para_range = odict([\n",
    "#     ('type', ['CuDNNLSTM']), # ('type', ['CuDNNGRU, CuDNNLSTM, GRU, LSTM']),\n",
    "#     ('batch_size', [128]),\n",
    "#     ('val_split', [0.05]), # validation / train split\n",
    "#     ('maxlen', [50]), # network sequence length\n",
    "#     ('layer', [2]),\n",
    "#     ('unit', [512])\n",
    "# ])\n",
    "# run_eval(run_train(para_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter Tuning\n",
    "# para_range = odict([\n",
    "#     ('type', ['CuDNNGRU']), \n",
    "#     ('batch_size', [128]),\n",
    "#     ('val_split', [0.05]),\n",
    "#     ('maxlen', [50]),\n",
    "#     ('layer', [2, 3, 4, 5]),\n",
    "#     ('unit', [128, 192, 256, 320])\n",
    "# ])\n",
    "# train_csv = run_eval(run_train(para_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Model Synthesis \n",
    "# prime = \"The answer to life, the universe and everything is\"\n",
    "# run_prime(prime, train_csv, 9, 10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
